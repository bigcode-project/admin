This repository serves as place for administrative things and a place for generic todos and issues. If you're new to BigCode, we recommend reading the section below to see how you can contribute to the project. 

# What has been done and how can you help?

üìö**Dataset:** We‚Äôve collected 130M+ Github repositories and ran a license detector on them. We will soon release a large dataset with files from repositories with permissive licenses. Besides Github, we would like to add more datasets and create a Code Dataset Catalogue. 

Open tickets:
* [Suggest datasets for the Code Dataset Catalogue](https://github.com/bigcode-project/admin/issues/15)
* [Redact Personally Identifiable Information (PII) from code datasets](https://github.com/bigcode-project/admin/issues/9)

We encourage you to join #wg-dataset if you are interested in discussions about data governance (e.g. regarding the ethical and legal concerns of the training data, OpenRAIL licenses for code applications, etc). 

üïµüèª‚Äç‚ôÄÔ∏è**Evaluation:** We started working on an [evaluation harness](https://github.com/bigcode-project/bigcode-evaluation-harness) to evaluate code generation models in an easy way on a wide range of tasks.

Open tickets:
* [Suggest tasks for the Evaluation Harness](https://github.com/bigcode-project/admin/issues/17)
* [Add selected tasks to the Evaluation Harness](https://github.com/bigcode-project/admin/issues/16)
* [Design prompts for few-shot evaluation tasks](https://github.com/bigcode-project/admin/issues/19) 

Please join #wg-evaluation for all discussions on the evaluation of code LLMs.

üí™**Training:** We‚Äôve been training smaller models (350M-1B parameters) on the ServiceNow cluster through [a fork of Megatron-LM](https://github.com/bigcode-project/Megatron-LM). 
* We‚Äôve ported [ALiBi](https://arxiv.org/abs/2108.12409)  in order to support longer sequences at inference time
* We‚Äôve implemented [multi-query attention](https://arxiv.org/abs/1911.02150) so as to speed-up incremental decoding
* The goal is to scale to a ~15B parameter model. We will, however, first run several ablation studies on a smaller scale. We will soon release our experiment plan and ask for your feedback!

We encourage you to get in touch with us at #wg-training if you have experience with large-scale transformer training in a multi node setup. 

üèé **Inference:** We‚Äôve implemented [multi-query attention](https://arxiv.org/abs/1911.02150) in Transformers and Megatron-LM. While others have[ reported up to a 10x decoding speed-up](https://arxiv.org/abs/1911.02150) over a multi-head attention baseline, [we‚Äôve only seen modest improvements of ~25%](https://github.com/bigcode-project/transformers/issues/1). 

Open tickets:
* [Improve inference speed of multi-query attention model ](https://github.com/bigcode-project/admin/issues/18)

Please go to the #wg-inference channel for technical discussion on how to improve inference speed of LLMs. You can find a summary of all open tickets [here](https://github.com/orgs/bigcode-project/projects/1/views/1?filterQuery=label%3A%22help+wanted%22). 

# Overview of our repositories

## [Megatron-LM](https://github.com/bigcode-project/Megatron-LM)
This repository contains the Megatron-LM fork used for training the BigCode models.

## [bigcode-analysis](https://github.com/bigcode-project/bigcode-analysis)
The BigCode analysis repository is a place for all kinds of analysis reports.

## [bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness)
The BigCode evaluation harness is developed to evaluate language models for code on several benchmarks.

## [bigcode-website](https://github.com/bigcode-project/bigcode-website)
The BigCode website contains the source of the website.
